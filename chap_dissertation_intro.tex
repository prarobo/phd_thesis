%=========================================================================================
% Introduction

\chapter{Introduction}
\label{chap:thesis_intro}

%================================================================================================================
\section{Benefits of Robotic Systems}

Robotic systems are useful for tasks that come under the purview of the ``\emph{3D}'s''--dirty, dull and dangerous. 
Currently, manufacturing and assembly lines, where tasks are of repetitive nature, are places where robotic systems are ubiquitous. ``Dirty'' tasks like traversing tunnels or sewers are being slowly delegated to tele-operated robots. 
The other avenue where robotic systems are being promoted relates to activities deemed dangerous for humans, like deep sea exploration, biologically, or chemically contaminated zones, and conflict areas. With enabling new advances in the field of robotics, robots are beginning to take over new roles to complement and assist humans in various ways.

Though advances in robotic systems have enabled robots to outperform humans in certain complicated tasks that are usually considered to require intelligence, such as competing in games like Go \cite{deepmind} and Chess \cite{deepblue}, 
robotic systems still lack the ability to reason and operate
autonomously in most unpredictable real world environments. 
In such cases, a human expert is deemed necessary to make decisions for the robot and guide the machine
to accomplish its objectives. Another domain where tele-operation is prevalent is in operations in inhospitable environments. Some prime examples of such remote operation in
dangerous environments include the rescue and repair effort at radiation affected zones in Fukushima Daichii nuclear power plant \cite{fukushima}, or combat operations via remotely operated weapons like Packbot \cite{packbot} and Predator \cite{predator} that have been deployed
at war-zones in Iraq and Afghanistan. Though tele-operation appears to offer a solution, a more scalable alternative
is aiming for full autonomy of robotic systems to minimize the strain and demands on human workforce.

%================================================================================================================
\section{Tele-operation}

Tele-operation offers an avenue for humans to leverage the benefits posed by a robotic system to perform activities 
that are \emph{dirty} or \emph{dangerous} without being physically present on the site of operation. During tele-operation, the human operator 
typically operates the robot from a safe distance away from the point of activity of the robot. This safe distance could range from a few meters to over a few hundred kilometers, based on the nature of the activity. For instance, in case of a tunnel or sewer inspection, the operator can stay a few meters outside the structure and drive a robot that is traversing inside. On the other hand, in case of weaponized drones like the predator \cite{predator}, the operator typically controls the drone from a safe remote command center, several hundred kilometers from the point of action of the drone. Tele-operation allows the operator to process the sensory information collected by the robot and make \emph{informed decisions} for the robot. The robot then translates an \emph{informed decision} into a sequence of actions. The tele-operation solution allows a human operator to safely accomplish an objective 
without being in discomfort or risk. 

Tele-operation brings together durability of a robotic agent and human intelligence to provide a powerful solution to solve problems ranging from deep sea exploration to safe operation in combat zones. Despite this advantage, tele-operation still requires the constant attention and involvement of a human operator. Additionally, since the human operator is not present, a reliable communication channel is essential to remotely control the robot. If the communication links are unreliable, or subject to willful sabotage by an enemy in a war-zone, the robotic system can get disconnected from the human operator. Such communication interruptions could possibly result is mission failure. The constant need for a human operator and the need for reliable communication channels are some of the shortcomings that limit the applicability of tele-operated systems.

If the objective is to move towards decreasing the strain on human workforce, it is imperative to decrease the reliance of robots on humans for operation. This emphasizes the need for automated decision-making in robotic systems. Furthermore, building automated, decision-making capable robotic systems obviates the need for reliable communication channels. Thus improving the cognitive capabilities to enable automated decision-making is essential to build robust self-sufficient robotic systems.

%================================================================================================================
\section{Components of Automated Informed Decision-Making}

The first component of informed decision-making is gathering the information needed to make a decision. 
For a robotic system,
information about the environment and the objective it needs to accomplish, dictate the decisions it makes. 
Collecting sensory data, is the prime 
and often the only mechanism, available for a robot to learn about its environment. Based on the 
type of sensors, different measurements describing the state of the environment are available. 
These raw sensor data need to be processed to get actionable information that enable a robot to reason about the state of its environment. 
Then decision may be seen as extrapolation of the reasoning process to choose an action from the set of actions available to a system.
If the entire sequence, from sensory data aggregation to choosing an action based on processed information, is performed by
a robotic system without any external intervention, then the due process can be characterized as automated, informed, decision-making. 

Transforming sensory data into usable information involves parsing the data to gain a semantic understanding about the state of an environment.
The basic component in understanding a scene\footnote{Scene here is defined as the state of an environment at a given point in time.} is separation and labeling of different objects present.
A more refined version of this understanding is deciphering the relationship between the objects in a scene.
Such a detailed semantic understanding allows a robot to infer the ability of the scene elements 
to actively participate in the robot's planned actions.

The ability of a robot to detect and label objects in its environment is an important step towards gaining semantic understanding of its environment.
Since this semantic understanding of a scene dictates the ability of a robotic system to make informed decisions, object recognition capability
is central to automated informed decision-making.

%================================================================================================================
\section{Automated Decision-Making in Natural Environment}

Building robotic systems that are fully autonomous, with an ability to make informed decisions based on their observations under all possible environmental conditions, is an open problem.
The existing solutions typically apply to certain specialized domains only. 
Before further discussion on this topic, it is important to
understand that natural environments pose significant challenges that compound the problems faced by robotic systems operating in other regimes.
Section~\ref{sec:nat_environ_def} states the definition of natural environment in the context of this dissertation and, Section~\ref{sec:robots_nat_environ} goes into the details on the challenges faced by robotic systems in natural environments.


\subsection{Natural Environment}
\label{sec:nat_environ_def}

The word \emph{Natural Environment} in this dissertation refers to uncontrolled and unstructured real world environments, with scenes containing predominantly naturally occurring objects; the robot has 
no direct control over any environment parameter. (However we assume that the robot can use accessories, like artificial light sources, to influence the state of the environment and thereby enhance sensor measurements.) 
Natural environments are typically challenging to model due to the unpredictability in the environment parameters and unknown noise parameters. 
Deep sea or forest landscapes with vegetation are some prime examples of such natural environments.


\subsection{Robotic Systems in Natural Environments}
\label{sec:robots_nat_environ}

In the past, robots have been specialized to do tasks in well defined environments, like assembly lines, where environmental variables (e.g. visibility and lighting) are strictly controlled. If such environment variables are known in advance, a mathematical model that maps robot actions to finite known environment states can be built. In such cases, based on the state of the environment observed, a robot action that complies with a predefined database of logical rules is chosen. Systems that operated on this principle, sometimes referred to as \emph{Expert Systems} 
\cite{russel}, came into existence in 1980's. Such systems fail in scenarios where the environment cannot be perfectly modeled. Despite the advances in field of robotics since then, robotic systems still face challenges when dealing with unpredictable environments similar to those in natural settings found in forests or deep sea.

The task of understanding natural scenes and recognizing objects like animals from them is shown to be a cognitively challenging task 
even for humans \cite{wichmann}. The unstructured nature and unpredictability associated with natural environment makes the task of 
building models to capture natural scenes challenging.
Using global features to characterize the nature of a scene is, in general, possible \cite{olivia}. However, these models can only offer a high level understanding
of the scene. For instance, the high level understanding translates to differentiating between widely different scenes like a beach, foliage or a busy city street. It does not inform about separating the different components or objects in those scenes. Most object recognition techniques depend on features like edges or texture to identify such objects. Such feature-based approaches are difficult to implement in natural scenes where the edges of objects can be difficult to distinguish.
The measurement noise in such natural scenes further complicates the object recognition task. 
Thus, for a robotic system to be
successful in making automated informed decisions in natural environments, it has to possess robust object recognition capabilities that can handle noise and variations in environmental conditions.


%================================================================================================================
\section{Noise in Sensor Measurements}

Sensors are designed to measure a physical quantity. This physical quantity is captured by the sensor in the form of a signal.
The measurement reported by the sensor almost always does not exactly match the real value of the physical quantity.
This error in the measured value is considered as measurement noise.
The ubiquitous presence of noise in almost all sensor measurements has spurred a field of study in the name of 
\emph{Filtering} \cite{haykin}. Filtering deals with the estimation of signals from noisy data. 
If the source and nature of noise that affects measurements of a sensor can be somehow modeled, then specialized
filters can be designed to recover the underlying signal from noisy sensor measurements.
The challenge here is in identifying the noise sources and modeling them. 
The sources of noise in sensor data can be manifold. An exhaustive discussion on sources of noise and methods to model them
is beyond the scope of this dissertation. In the remainder of this section, we will focus on specific sources of noise that affect 
imagery data collected by robots operating in natural environments (Section~\ref{sec:noise_sources}) and some filters that can 
help recover the underlying signal in these cases (Section ~\ref{sec:noise_filters}).

\subsection{Noise Sources}
\label{sec:noise_sources}

The sources of noise associated with a particular  application need to be carefully analyzed before designing appropriate filters to mitigate this noise.
In the specific case of imaging applications that employ a camera sensor in natural environments, the various noise sources can be broadly divided
into \begin{enumerate*}[label=(\roman*)] \item noise inherent to the sensor, \item noise associated with data collection setup, and \item noise introduced by the environment. \end{enumerate*} For instance, in the case of an underwater imaging application using an \gls{auv}, the noise associated with the imaging setup constitutes noise introduced due to vibration in the motion of the \gls{auv}. Additionally, the environmental parameters associated with deep sea environments could introduce noise which could further degrade the sensor measurements. Further details about the different noise sources of this type is discussed in the rest of this section.

In an imaging application, some noise is inherent to the camera sensor. Studies \cite{irie} have analyzed the various constituents of noise 
in a \gls{ccd} video-camera. According the these studies, the sources of noise in a camera can further be divided into three categories, namely: illumination-independent noise, illumination-dependent noise, and digital processing noise. After further analysis, this work suggests that there are four major noise sources that contribute to the camera sensor noise. Out of these four sources, \emph{readout noise} and \emph{fixed pattern noise} are constant illumination-independent noise sources. One of the factors that determine the value of readout noise and fixed pattern noise is the temperature of the sensor. The remaining two primary contributing noise sources, \emph{photo-shot noise} and \emph{photo-response non-uniformity noise} are illumination-dependent noise sources. Its important to note that, in illumination dependent noise sources the intensity of noise is a function of the intensity value of the pixel. Modeling all the noise sources for different camera sensors poses a challenge in itself.

Another source of noise is the physical setup which holds the camera. This setup could be an \gls{auv} or just a stationary imaging rig. Such physical structures that hold the camera can introduce noise in form of vibration or motion of the structure during imaging. Such motion could lead to out-of-focus or blurred pictures. There could also be cases where the physical setup can produce artifacts like shadows that could affect the sensor measurements. The few examples mentioned here are not all encompassing. Each imaging setup needs to be studied to identify the possible sources of noise it can introduce into sensor measurements.

Environmental conditions are one of the chief sources of noise. 
There is a key difference between noise due to environmental conditions 
and other sources like sensor noise or the imaging setup noise: the former is relatively more difficult to model compared to the latter due to wide range of variables associated with environment. 
These environmental variables are fairly limited in controlled environments, but in case of natural environments
the number of variables are far higher. Furthermore, the ability to predict or model the environmental variables in natural
environments is far more challenging compared to controlled environments. To understand some of the variables associated with natural environments, let us consider the case of imaging experiments in underwater environments. 
Some of the variables at play here are non-uniform low illumination, absorption of light by water,
and scattering of light by different particles in water. Each of these parameters can have considerable impact on the quality of the image acquired by
a camera. Modeling these environment variables and accounting for the noise induced by them in the sensor measurements is critical to extract useful
information from the sensor data.

Any object recognition method has to account for the different noise sources that can affect sensor measurements. Especially in case of natural environments, environmental conditions typically act as major noise sources. 
In any case, it is important to take appropriate steps to mitigate the effect of noise 
in the sensor measurements before useful information can be extracted. 
Even in the absence of accurate models of different noise sources, it is important to attempt to model the noise seen in the sensor data
and devise filters to extract the underlying signal from recorded sensor measurements.

\subsection{Noise Filtering}
\label{sec:noise_filters}

The sensor noise model is often a part of the sensor specification provided by the manufacturer. 
Thus the person integrating the sensor into a custom application, can sometimes avoid modeling the sensor noise. 
However, the noise induced by the setup is unique to each system and hence needs to be analyzed to identify the characteristics of noise.
Once a model is identified, the subsequent filter design involves designing a filter that can recover the signal from recorded sensor measurements.

The task of filtering noise induced by the environment is more complex relative to that
of sensor or setup.
The reason for this is the difficulty in modeling environmental variables. 
In the case of underwater imaging applications, understanding the physics of 
light propagation, that includes phenomena like absorption and scattering, is required. Some proposed models \cite{garcia, ahlen} try to 
capture light propagation in water and also suggest filter models that can correct illumination and color in noisy underwater images. A more detailed survey
by Jaffe \cite{jaffe} describes different existing filter models available to deal with noise in underwater images.
In most cases, modeling all the environment parameters is intractable. Hence identifying the dominant noise source or simply identifying the nature of noise affecting the images are possible alternatives.
For instance, in scallop recognition \cite{prasanna_aslo}, the \gls{auv} images used were corrupted by speckle noise, hence a median filter \cite{despeckle} was used to minimize noise without elaborate modeling of noise sources.

Since extracting useful information from sensor data is essential for object recognition, noise filters constitute a core component of object recognition systems. The first stage in noise filtering involves identifying noise sources and modeling them. Accurate modeling of noise sources in often challenging, hence most systems use simplified models that only capture dominant noise sources. Since there are several filtering mechanisms available, filter design often involves human intuition to pick appropriate filters that can handle the noise pattern seen. Ultimately, noise filtering allows distilling useful information for informed decision-making.

%================================================================================================================
\section{Object Recognition in Natural Environments}

For an object recognition framework to be effective in natural environments, it
should be robust to noise and variations in environmental conditions. 
To avoid dealing with uncertainties in environmental conditions, researchers tend to specialize their object recognition technique to work in certain controlled conditions. In some instances even the sensing apparatus used can be explicitly designed to detect a certain organism as in the case of plankton 
recognitions systems \cite{mcgavin_plankton, stelzer_rotifier}. 
Such techniques, that are highly specialized to solve a specific problem, 
often do not transfer to other application domains. An example of object recognition in controlled conditions is the scallop recognition system designed to work in artificial scallop beds \cite{enomoto9,enomoto10}. In this method, a series of stationary cameras under known conditions are used to perform object recognition. The object recognition systems built on the assumption of controlled environmental conditions, generally do not translate well to natural environments that exhibit wide variation in environmental conditions.

Building specialized object recognition frameworks that translate between different application domains is challenging. 
At the same time, it is equally challenging to build a generalized object recognition method that can accommodate wide variations in environmental conditions. 
One approach to this problem is the use of \emph{multi-layered} object recognition frameworks. Multi-layered object recognition frameworks with dedicated modules for dealing with different sub-problems, like filtering and hypothesis testing, allow easy customization of layers based on application requirements. 
Thus multi-layered frameworks allow researchers to configure specialized layers that are custom designed for their object recognition problem. The scallop
recognition framework \cite{prasanna_aslo, prasanna_igi} offers one such multi-layered framework. Another interesting example is the set of underwater object recognition tools \cite{schoening} used for recognizing organisms like sea-anemones.

In conclusion, multi-layered object recognition frameworks offer a flexible solution to automated object recognition problems. Customizing a multi-layered object recognition framework to work with a specific problem amounts to picking the right layers that would be effective in solving the problem. The layers used can be adaptations of existing computer vision or machine learning tools. For cases where there is no such off-the-shelf solution, specialized layers can be engineered to fit the problem. A researcher can thus build a multi-layered framework with existing tools in tandem with custom designed solutions to solve specialized object recognition applications.

%================================================================================================================
\section{Object Recognition as a path to Robot Autonomy}

Object recognition capabilities allow a robotic system to identify and label the different components in its environment. Such understanding of individual objects in the robot's surrounding is required to build a semantic model that captures the relationship between the different components in an environment.
Cognitive reasoning relies on such semantic models to determine the best possible action for a robot given its knowledge about the state of the environment. This is why object recognition capabilities are essential to building robotic systems capable of automated informed decision-making.

The task of object recognition and understanding the state of the environment becomes challenging in the presence of noise. Sensor measurements
that are supposed to measure the state of the environment could be corrupted due to the presence of noise. In order to avoid being misled by bad measurements,
a robotic system needs to employ noise filters to extract the underlaying signal from sensor measurements. Noise filters become a core component of the perception system that aggregates sensor measurements to gain knowledge about the state of the environment. Building accurate models of the surroundings despite the presence of noise is key to the success of a robotic system that is intended to operate autonomously in natural environments.

In conclusion, object recognition capabilities, with robustness to noise, are required for building systems that operate in natural environments. Object recognition, coupled with an ability to model and reason about the state of an environment, allows a robotic system to make automated informed decisions. Automated informed decision-making, in conjunction with autonomous actuation, paves the way for robot autonomy. Autonomous systems can act independently without any human support in a range of environments. Such systems can ease the strain on human work force by taking up tasks that are physically challenging or risky for a human to perform.

%================================================================================================================
\section{Approach Overview}

Given that object recognition in noisy environments is critical for robot autonomy, this dissertation proposes a series of techniques designed to address the object recognition problem in noisy environments.
We choose
underwater object recognition as the primary domain to conduct the experimental validation for this approach.
The rest of this section offers a brief overview of the different object recognition techniques developed as a part of this overall approach, along with insights on the particular problems they address.

Recognizing submerged subway cars from seabed images was the first application that motivated the development of an object recognition technique capable of operating in noisy seabed images. Since recognizing subway cars can be reduced 
to detecting rectangles, eigen-value based shape descriptors with in-built \gls{rst} invariance, were tested
to solve this specific object recognition problem. Eigen-value based shape descriptors were successful in detecting 
simple shapes like rectangles from images.
During the course of the study, several shortcomings of eigen-value shape descriptors were exposed. These include poor performance in noisy domains, and inability to identify objects characterized by complex shape profiles given constraints on image discretization. Chapter~\ref{chap:eigen} of this dissertation offers a detailed treatment of the eigen value shape descriptors, and the results of the subway car recognition problem they were tested on.

When the scope of the object recognition problem was expanded from simple shapes like rectangles to underwater organisms characterized by complex profiles,
eigen-value shape descriptors were no longer a viable choice. The primary application driver for the design of the multi-layered object recognition system in Chapter~\ref{chap:scallop_recog} was the need for a 
scallop recognition system in image-based benthic survey efforts capable of automated scallop enumeration. 
The multi-layered object recognition system thus designed was capable of recognizing objects from noisy natural images. This method detected 60--70\% of scallops, present in a dataset of over 8000 images. One important point here is that the method was explicitly designed to deal with noise introduced by non-uniform lighting, low resolution, and large levels of speckle noise, that previous scallop recognition efforts were not equipped to handle. Chapter~\ref{chap:scallop_recog} elaborates on the multi-layered object recognition framework developed to meet this challenge.

Despite this multi-layered object recognition framework proposed in Chapter~\ref{chap:scallop_recog}, being able to detect objects from noisy images, there were 
several instances of false positives in the ensuing detections. This prompted the need for a classification technique that can recognize objects with smaller percentage of
false positives. To accomplish this we merged information from multiple views of an object before determining its identity. The details involved in this
multi-view object recognition approach are discussed in Chapter~\ref{chap:distdes}.

During the development of these object recognition algorithms, the need for a low-cost research platform 
that can be used as a test bed to evaluate different object recognition algorithms was realized.
In response to this, CoopROV, a low cost underwater \gls{rov} was developed. CoopROV's specifications and 
design procedure is discussed in Chapter~\ref{chap:cooprov}.

%================================================================================================================
\section{Dissertation Organization}

Firstly, Chapter~\ref{chap:eigen} discusses the eigen-value shape descriptors and the subway car detection problem they were evaluated on.
The multi-layered object recognition technique developed to support an automated scallop survey effort 
is then discussed in Chapter~\ref{chap:scallop_recog}.
This is followed by Chapter~\ref{chap:distdes}, that describes a multi-view object recognition technique that allows to combine information from multiple views of a target object, and decreases the number of false positives seen in the multi-layered object recognition technique.
Chapter~\ref{chap:cooprov} provides details on the underwater \gls{rov} designed as a research prototype to test object recognition algorithms.
Finally, Chapter~\ref{chap:thesis_conclusion} highlights the insights gleaned during the development of the different object recognition techniques along with possible directions that can be explored in the future.

